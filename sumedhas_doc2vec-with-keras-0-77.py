from __future__ import print_function

import os

import re

import tqdm

import string

import pandas as pd

import numpy as np

import keras
""" Read Data """

train_variant = pd.read_csv("../input/training_variants")

test_variant = pd.read_csv("../input/test_variants")

train_text = pd.read_csv("../input/training_text", sep="\|\|", engine='python', header=None, skiprows=1, names=["ID","Text"])

test_text = pd.read_csv("../input/test_text", sep="\|\|", engine='python', header=None, skiprows=1, names=["ID","Text"])

train = pd.merge(train_variant, train_text, how='left', on='ID')

train_y = train['Class'].values

train_x = train.drop('Class', axis=1)

train_size=len(train_x)

print('Number of training variants: %d' % (train_size))

# number of train data : 3321



test_x = pd.merge(test_variant, test_text, how='left', on='ID')

test_size=len(test_x)

print('Number of test variants: %d' % (test_size))

# number of test data : 5668



test_index = test_x['ID'].values

all_data = np.concatenate((train_x, test_x), axis=0)

all_data = pd.DataFrame(all_data)

all_data.columns = ["ID", "Gene", "Variation", "Text"]
all_data.head()
from nltk.corpus import stopwords

from gensim.models.doc2vec import LabeledSentence

from gensim import utils



def constructLabeledSentences(data):

    sentences=[]

    for index, row in data.iteritems():

        sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))

    return sentences



def textClean(text):

    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)

    text = text.lower().split()

    stops = set(stopwords.words("english"))

    text = [w for w in text if not w in stops]    

    text = " ".join(text)

    return(text)

    

def cleanup(text):

    text = textClean(text)

    text= text.translate(str.maketrans("","", string.punctuation))

    return text



allText = all_data['Text'].apply(cleanup)

sentences = constructLabeledSentences(allText)

allText.head()
sentences[0]
from gensim.models import Doc2Vec



Text_INPUT_DIM=300





text_model=None

filename='docEmbeddings_5_clean.d2v'

if os.path.isfile(filename):

    text_model = Doc2Vec.load(filename)

else:

    text_model = Doc2Vec(min_count=1, window=5, size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, iter=5,seed=1)

    text_model.build_vocab(sentences)

    text_model.train(sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)

    text_model.save(filename)
text_train_arrays = np.zeros((train_size, Text_INPUT_DIM))

text_test_arrays = np.zeros((test_size, Text_INPUT_DIM))



for i in range(train_size):

    text_train_arrays[i] = text_model.docvecs['Text_'+str(i)]



j=0

for i in range(train_size,train_size+test_size):

    text_test_arrays[j] = text_model.docvecs['Text_'+str(i)]

    j=j+1

    

print(text_train_arrays[0][:50])
from sklearn.decomposition import TruncatedSVD

Gene_INPUT_DIM=25



svd = TruncatedSVD(n_components=25, n_iter=Gene_INPUT_DIM, random_state=12)



one_hot_gene = pd.get_dummies(all_data['Gene'])

truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)



one_hot_variation = pd.get_dummies(all_data['Variation'])

truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values)
from keras.utils import np_utils

from sklearn.preprocessing import LabelEncoder



label_encoder = LabelEncoder()

label_encoder.fit(train_y)

encoded_y = np_utils.to_categorical((label_encoder.transform(train_y)))

print(encoded_y[0])
train_set=np.hstack((truncated_one_hot_gene[:train_size],truncated_one_hot_variation[:train_size],text_train_arrays))

test_set=np.hstack((truncated_one_hot_gene[train_size:],truncated_one_hot_variation[train_size:],text_test_arrays))

print(train_set[0][:50])
from keras.models import Sequential

from keras.layers import Dense, Dropout, LSTM, Embedding, Input, RepeatVector

from keras.optimizers import SGD



def baseline_model():

    model = Sequential()

    model.add(Dense(256, input_dim=Text_INPUT_DIM+Gene_INPUT_DIM*2, init='normal', activation='relu'))

    model.add(Dropout(0.3))

    model.add(Dense(256, init='normal', activation='relu'))

    model.add(Dropout(0.5))

    model.add(Dense(80, init='normal', activation='relu'))

    model.add(Dense(9, init='normal', activation="softmax"))

    

    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)  

    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

    return model
model = baseline_model()

model.summary()
estimator=model.fit(train_set, encoded_y, validation_split=0.2, epochs=10, batch_size=64)
print("Training accuracy: %.2f%% / Validation accuracy: %.2f%%" % (100*estimator.history['acc'][-1], 100*estimator.history['val_acc'][-1]))
import matplotlib.pyplot as plt



# summarize history for accuracy

plt.plot(estimator.history['acc'])

plt.plot(estimator.history['val_acc'])

plt.title('model accuracy')

plt.ylabel('accuracy')

plt.xlabel('epoch')

plt.legend(['train', 'valid'], loc='upper left')

plt.show()



# summarize history for loss

plt.plot(estimator.history['loss'])

plt.plot(estimator.history['val_loss'])

plt.title('model loss')

plt.ylabel('loss')

plt.xlabel('epoch')

plt.legend(['train', 'valid'], loc='upper left')

plt.show()
y_pred = model.predict_proba(test_set)
submission = pd.DataFrame(y_pred)

submission['id'] = test_index

submission.columns = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6', 'class7', 'class8', 'class9', 'id']

submission.to_csv("submission_all.csv",index=False)

submission.head()
from keras import backend as K

import seaborn as sns



layer_of_interest=0

intermediate_tensor_function = K.function([model.layers[0].input],[model.layers[layer_of_interest].output])

intermediate_tensor = intermediate_tensor_function([train_set[0,:].reshape(1,-1)])[0]

import matplotlib

colors = list(matplotlib.colors.cnames)



intermediates = []

color_intermediates = []

for i in range(len(train_set)):

    output_class = np.argmax(encoded_y[i,:])

    intermediate_tensor = intermediate_tensor_function([train_set[i,:].reshape(1,-1)])[0]

    intermediates.append(intermediate_tensor[0])

    color_intermediates.append(colors[output_class])
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=0)

intermediates_tsne = tsne.fit_transform(intermediates)

plt.figure(figsize=(8, 8))

plt.scatter(x = intermediates_tsne[:,0], y=intermediates_tsne[:,1], color=color_intermediates)

plt.show()